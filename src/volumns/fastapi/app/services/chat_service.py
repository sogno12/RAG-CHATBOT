# app/services/chat_service.py
from fastapi import HTTPException
from typing import Union

from app.services.search_service import search_similar_docs
from app.services.llm_service import call_llm, call_llm_with_logging, check_llm_connection, get_llm_models
from app.services.session_service import get_history, save_message, update_session_summary, get_session_summary
from app.utils.logger import logger

def llm_health_check() -> dict:
    if check_llm_connection():
        return {"status": "ok", "message": "LLM ì„œë²„ ì—°ê²° ì„±ê³µ"}
    else:
        raise HTTPException(status_code=503, detail="LLM ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def get_llm_status_verbose() -> dict:
    return get_llm_models()


def build_prompt(query: str, docs: list[str], history: Union[str, list[dict]]) -> str:
    # 1. history_text ìƒì„± ë°©ì‹ ë¶„ê¸°
    if isinstance(history, str):
        history_text = history
    elif isinstance(history, list):
        history_text = "\n".join(f"{msg['role']}: {msg['content']}" for msg in history)
    else:
        history_text = "[ì´ë ¥ ì—†ìŒ]"

    # 2. ë¬¸ì„œ í…ìŠ¤íŠ¸ ì •ë¦¬
    docs_text = "\n".join(docs)

    return f"""
[Document Context]
{docs_text}

[Conversation History]
{history_text}

[User Question]
{query}
""".strip()


async def chat_with_context(query: str, history: list[dict]) -> dict:
    # 1. ë²¡í„° ê²€ìƒ‰
    retrieved_docs = search_similar_docs(query)

    # 2. ì´ì „ ëŒ€í™”ê¸°ë¡ + ê²€ìƒ‰ ê²°ê³¼ë¡œ LLM í˜¸ì¶œ
    prompt = build_prompt(query, retrieved_docs, history)
    answer = await call_llm_with_logging(query, prompt)

    return {
        "answer": answer,
        "context_docs": retrieved_docs
    }


async def chat_with_session(user_id: str, session_id: str, query: str) -> dict:

    # 1. ì„¸ì…˜ ì´ë ¥ ì¡°íšŒ
    history = get_history(user_id, session_id)

    # 2. ë²¡í„° ê²€ìƒ‰
    retrieved_docs = search_similar_docs(query)
    logger.info(f"ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ (ì´ {len(retrieved_docs)}ê°œ ë¬¸ì„œ)")

    # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = build_prompt(query, retrieved_docs, history)
    logger.info(f"ğŸ§± í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(prompt)}ì)\ní”„ë¡¬í”„íŠ¸ ì¼ë¶€:\n{prompt[:30]}...")

    # 4. LLM í˜¸ì¶œ
    answer = await call_llm_with_logging(query, prompt)

    # 5. ì„¸ì…˜ ì €ì¥
    save_message(user_id, session_id, {"role": "user", "content": query})
    save_message(user_id, session_id, {"role": "assistant", "content": answer})

    return {
        "query": query,
        "context_docs": retrieved_docs,
        "answer": answer
    }

async def chat_with_summary(user_id: str, session_id: str, query: str) -> dict:

    # 1. ì„¸ì…˜ ìš”ì•½ ì •ë³´ ê²€ìƒ‰
    summary = get_session_summary(user_id, session_id)
    logger.info(f"ğŸ§  ì„¸ì…˜ ìš”ì•½ ì •ë³´ ì¡°íšŒ ì™„ë£Œ: {summary[:30]}...")

    # 2. ë²¡í„° ê²€ìƒ‰
    retrieved_docs = search_similar_docs(query)
    logger.info(f"ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ (ì´ {len(retrieved_docs)}ê°œ ë¬¸ì„œ)")

    # 3. í”„ë¡¬í”„íŠ¸ ìƒì„± (ìš”ì•½ ê¸°ë°˜ context ì‚¬ìš©)
    prompt = build_prompt(query, retrieved_docs, summary)
    logger.info(f"ğŸ§± í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(prompt)}ì)\ní”„ë¡¬í”„íŠ¸ ì¼ë¶€:\n{prompt[:30]}...")

    # 4. LLM í˜¸ì¶œ
    answer = await call_llm_with_logging(query, prompt, user_id=user_id, session_id=session_id)

    # 5. ì„¸ì…˜ ì €ì¥
    save_message(user_id, session_id, {"role": "user", "content": query})
    save_message(user_id, session_id, {"role": "assistant", "content": answer})

    # 6. ê²°ê³¼ ë¦¬í„´
    return {
        "query": query,
        "context_docs": retrieved_docs,
        "answer": answer
    }


def summarize_and_update_session(user_id: str, session_id: str, query: str, answer: str):
    try:
        history = get_history(user_id, session_id)
        summary = summarize_messages(
            history + [{"role": "user", "content": query}, {"role": "assistant", "content": answer}]
        )
        update_session_summary(user_id, session_id, summary)
        logger.info(f"[ìš”ì•½ ì„±ê³µ] {session_id} - {summary}")
    except Exception as e:
        logger.warning(f"[ìš”ì•½ ì‹¤íŒ¨] {session_id} - {e}")


# ìš”ì•½ í•¨ìˆ˜
def summarize_messages(history: list[dict]) -> str:
    conversation = "\n".join(f"{m['role']}: {m['content']}" for m in history)
    prompt = f"[ëŒ€í™” ìš”ì•½]\në‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ë‹¨íˆ ìš”ì•½í•´ì¤˜:\n{conversation}"
    answer, _ = call_llm("", prompt)  # âœ… answerë§Œ ì‚¬ìš©
    return answer

