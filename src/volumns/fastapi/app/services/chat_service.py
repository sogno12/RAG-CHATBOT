# app/services/chat_service.py
from fastapi import HTTPException

from app.services.search_service import search_similar_docs
from app.services.llm_service import call_llm_with_timing, check_llm_connection, get_llm_models
from app.services.session_service import get_history, save_message
from app.services.log_service import log_llm_response
from app.utils.logger import logger

def llm_health_check() -> dict:
    if check_llm_connection():
        return {"status": "ok", "message": "LLM ì„œë²„ ì—°ê²° ì„±ê³µ"}
    else:
        raise HTTPException(status_code=503, detail="LLM ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def get_llm_status_verbose() -> dict:
    return get_llm_models()


def build_prompt(query: str, docs: list[str], history: list[dict]) -> str:
    history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history])
    docs_text = "\n".join(docs)

    return f"""
[Document Context]
{docs_text}

[Conversation History]
{history_text}

[User Question]
{query}
""".strip()


async def chat_with_context(query: str, history: list[dict]) -> dict:
    # 1. ë²¡í„° ê²€ìƒ‰
    retrieved_docs = search_similar_docs(query)

    # 2. ì´ì „ ëŒ€í™”ê¸°ë¡ + ê²€ìƒ‰ ê²°ê³¼ë¡œ LLM í˜¸ì¶œ
    prompt = build_prompt(query, retrieved_docs, history)
    answer, elapsed = call_llm_with_timing(query, prompt)

    # âœ… log ì €ì¥
    await log_llm_response(
        query=query,
        response=answer,
        elapsed=elapsed,
        context=prompt
    )

    return {
        "answer": answer,
        "context_docs": retrieved_docs
    }


async def chat_with_session(user_id: str, session_id: str, query: str) -> dict:

    # 1. ì„¸ì…˜ ì´ë ¥ ì¡°íšŒ
    history = get_history(user_id, session_id)

    # 2. ë²¡í„° ê²€ìƒ‰
    retrieved_docs = search_similar_docs(query)
    logger.info(f"ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ (ì´ {len(retrieved_docs)}ê°œ ë¬¸ì„œ)")

    # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = build_prompt(query, retrieved_docs, history)
    logger.info(f"ğŸ§± í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(prompt)}ì)\ní”„ë¡¬í”„íŠ¸ ì¼ë¶€:\n{prompt[:30]}...")

    # 4. LLM í˜¸ì¶œ
    answer, elapsed = call_llm_with_timing(query, prompt)

    # 5. ì„¸ì…˜ ì €ì¥
    save_message(user_id, session_id, {"role": "user", "content": query})
    save_message(user_id, session_id, {"role": "assistant", "content": answer})

    # âœ… log ì €ì¥
    await log_llm_response(
        query=query,
        response=answer,
        elapsed=elapsed,
        context=prompt,
        user_id=user_id,        # ì¶”ê°€
        session_id=session_id   # ì¶”ê°€
    )

    return {
        "query": query,
        "context_docs": retrieved_docs,
        "answer": answer
    }
