# app/services/llm_service.py
import requests
import os

VLLM_SERVER_URL = os.getenv("VLLM_SERVER_URL", "http://localhost:48000")

def check_llm_connection() -> bool:
    try:
        res = requests.get(f"{VLLM_SERVER_URL}/v1/models", timeout=5)
        return res.status_code == 200
    except Exception:
        return False

def call_llm(prompt: str) -> str:
    return f"ğŸ¤– (ëª¨ì˜ ì‘ë‹µ) '{prompt}' ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤."
