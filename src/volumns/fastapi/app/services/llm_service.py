# app/services/llm_service.py
import requests
import os

VLLM_SERVER_URL = os.getenv("VLLM_SERVER_URL", "http://localhost:48000")

def check_llm_connection() -> bool:
    try:
        res = requests.get(f"{VLLM_SERVER_URL}/v1/models", timeout=5)
        return res.status_code == 200
    except Exception:
        return False

def get_llm_models() -> dict:
    """LLM ì„œë²„ì˜ /v1/models ì „ì²´ ì‘ë‹µ ë°˜í™˜"""
    try:
        res = requests.get(f"{VLLM_SERVER_URL}/v1/models", timeout=5)
        res.raise_for_status()
        return res.json()
    except Exception as e:
        return {"status": "error", "message": f"LLM ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {e}"}

def call_llm(prompt: str) -> str:
    return f"ğŸ¤– (ëª¨ì˜ ì‘ë‹µ) '{prompt}' ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤."
