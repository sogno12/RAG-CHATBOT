services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm-server-sjchoi
    ports:
      - "48000:8000"  # í˜¸ìŠ¤íŠ¸:ì»¨í…Œì´ë„ˆ (ì™¸ë¶€ ì ‘ì†ì€ 48000ìœ¼ë¡œ)
    volumes:
      - /labs/docker/volumes/ml-dev/share/model:/models  # gemma ëª¨ë¸ ê²½ë¡œ
    command:
      - --model
      - /models/gemma2-9b-it  # ëª¨ë¸ ë””ë ‰í† ë¦¬
    environment:
      # ğŸ”¹ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ GPU 1ë²ˆë§Œ ì‚¬ìš©í•˜ê²Œ ì œí•œ (ë…¼ë¦¬ì  ì œì–´)
      - CUDA_VISIBLE_DEVICES=1

      # (ì„ íƒ ì‚¬í•­) ì•„ë˜ëŠ” NVIDIA ëŸ°íƒ€ì„ìš© ë³€ìˆ˜ - ë³´í†µ ìƒëµ ê°€ëŠ¥í•˜ì§€ë§Œ ì°¸ê³ ìš©ìœ¼ë¡œ ê¸°ì¬
      # - NVIDIA_VISIBLE_DEVICES=1      # â† ë¬¼ë¦¬ì ìœ¼ë¡œ ì»¨í…Œì´ë„ˆì—ì„œ GPU 1ë²ˆë§Œ ë³´ì´ê²Œ í•  ë•Œ ì‚¬ìš©
      # - NVIDIA_DRIVER_CAPABILITIES=all  # â† compute, utility ë“± ê¸°ëŠ¥ ì œí•œ í•´ì œ
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
